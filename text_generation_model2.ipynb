{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to build a model with transfer learning and word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten, Embedding, TimeDistributed, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import one_hot\n",
    "from pickle import dump\n",
    "\n",
    "import numpy as np \n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents \n",
    "with open('Reddit_Blonde_short.txt', 'r') as file: \n",
    "#with open(\"One_liners.txt\", 'r') as file:\n",
    "    doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A middle a\n"
     ]
    }
   ],
   "source": [
    "print(doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'middle', 'aged', 'frumpy', 'married', 'couple', 'return', 'to', 'a', 'mercedes', 'dealership', 'where', 'the', 'salesman', 'has', 'just', 'sold', 'the', 'car', 'they', 'were', 'interested', 'in', 'to', 'a', 'beautiful', 'leggy', 'busty', 'blonde', 'i', 'thought', 'you', 'said', 'you', 'would', 'hold', 'that', 'car', 'till', 'we', 'raised', 'the', 'asking', 'price', 'said', 'the', 'man', 'i', 'just', 'heard', 'you', 'close', 'the', 'deal', 'for', 'to', 'the', 'lovely', 'young', 'lady', 'there', 'you', 'insisted', 'there', 'could', 'be', 'no', 'discount', 'on', 'the', 'model', 'well', 'she', 'had', 'the', 'ready', 'cash', 'and', 'just', 'look', 'at', 'her', 'how', 'could', 'i', 'resist', 'replied', 'the', 'grinning', 'salesman', 'just', 'then', 'the', 'young', 'woman', 'approached', 'the', 'middle', 'aged', 'couple', 'and', 'gave', 'them', 'the', 'keys', 'there', 'you', 'go', 'she', 'said', 'i', 'told', 'you', 'i', 'would', 'get', 'the', 'prick', 'to', 'reduce', 'it', 'see', 'you', 'later', 'dad', 'a', 'blonde', 'arrives', 'at', 'work', 'one', 'morning', 'and', 'enters', 'the', 'elevator', 'with', 'a', 'coworker', 'she', 'smiles', 'at', 'him', 'and', 'says', 'tgif', 'the', 'coworker', 'smiles', 'back', 'and', 'says', 'shit', 'confused', 'the', 'blonde', 'says', 'again', 'tgif', 'once', 'again', 'her', 'coworker', 'says', 'shit', 'finally', 'the', 'blonde', 'says', 'thank', 'god', 'its', 'friday', 'the', 'coworker', 'responds', 'sorry', 'honey', 'its', 'thursday', 'blonde', 'why', 'are', 'all', 'the', 'blondes', 'rushing', 'to', 'get', 'breast', 'implants', 'because', 'they', 'dont', 'want', 'to', 'pay', 'the', 'flat', 'tax']\n",
      "Total Tokens: 16907\n",
      "Unique Tokens: 2386\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 16856\n",
      "\n",
      "a middle aged frumpy married couple return to a mercedes dealership where the salesman has just sold the car they were interested in to a beautiful leggy busty blonde i thought you said you would hold that car till we raised the asking price said the man i just heard you\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "#sequences = the text divided into len 50 \n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print()\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2387\n",
      "\n",
      "[2, 519, 1210, 2386, 1209, 518, 430, 4, 2, 2385, 2384, 276, 1, 160, 159, 58, 2382, 1, 69, 22, 51, 1208, 10, 4, 2, 192, 2377, 2375, 5, 9, 311, 8, 48, 8, 82, 1207, 17, 69, 2374, 39, 1206, 1, 837, 2372, 48, 1, 59, 9, 58, 275, 8]\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "print()\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(encoded_jokes)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# TAKES A LONG TIME TO LOAD\n",
    "# download pre-made embeddings \n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding \n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           238700    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2387)              241087    \n",
      "=================================================================\n",
      "Total params: 650,687\n",
      "Trainable params: 411,987\n",
      "Non-trainable params: 238,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(e)\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 [==============================] - 11s 82ms/step - loss: 0.0027 - accuracy: 0.1018\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 11s 86ms/step - loss: 0.0027 - accuracy: 0.1029\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 11s 86ms/step - loss: 0.0026 - accuracy: 0.1057\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 12s 91ms/step - loss: 0.0026 - accuracy: 0.1079\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 12s 92ms/step - loss: 0.0026 - accuracy: 0.1114\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 12s 93ms/step - loss: 0.0026 - accuracy: 0.1166\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 15s 115ms/step - loss: 0.0025 - accuracy: 0.1191\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 16s 124ms/step - loss: 0.0025 - accuracy: 0.1249\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.0025 - accuracy: 0.1318\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 18s 137ms/step - loss: 0.0024 - accuracy: 0.1392\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.0024 - accuracy: 0.1431\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.0024 - accuracy: 0.1501\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 18s 137ms/step - loss: 0.0023 - accuracy: 0.1570\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.0023 - accuracy: 0.1637\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 0.0022 - accuracy: 0.1714\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 17s 132ms/step - loss: 0.0022 - accuracy: 0.1792\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 18s 137ms/step - loss: 0.0022 - accuracy: 0.1887\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 19s 140ms/step - loss: 0.0021 - accuracy: 0.1952\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 18s 136ms/step - loss: 0.0021 - accuracy: 0.2056\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.0020 - accuracy: 0.2165\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 16s 122ms/step - loss: 0.0020 - accuracy: 0.2251\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 19s 140ms/step - loss: 0.0019 - accuracy: 0.2388\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 0.0019 - accuracy: 0.2463\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 0.0019 - accuracy: 0.2582\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.0018 - accuracy: 0.2685\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.0018 - accuracy: 0.2810\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.0017 - accuracy: 0.2919\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.0017 - accuracy: 0.3070\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 0.0017 - accuracy: 0.3164\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 18s 137ms/step - loss: 0.0016 - accuracy: 0.3312\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.0016 - accuracy: 0.3431\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.0015 - accuracy: 0.3608\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.0015 - accuracy: 0.3690\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.0015 - accuracy: 0.3840\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.0014 - accuracy: 0.3950\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 16s 119ms/step - loss: 0.0014 - accuracy: 0.4102\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 16s 120ms/step - loss: 0.0014 - accuracy: 0.4248\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.0013 - accuracy: 0.4378\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.0013 - accuracy: 0.4527\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.0013 - accuracy: 0.4685\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.0012 - accuracy: 0.4820\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.0012 - accuracy: 0.4951\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 17s 132ms/step - loss: 0.0012 - accuracy: 0.5091\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 18s 137ms/step - loss: 0.0011 - accuracy: 0.5220\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 18s 140ms/step - loss: 0.0011 - accuracy: 0.5367\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 0.0011 - accuracy: 0.5473\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 19s 141ms/step - loss: 0.0010 - accuracy: 0.5649\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 0.0010 - accuracy: 0.5778\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 18s 140ms/step - loss: 9.8234e-04 - accuracy: 0.5876\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 16s 121ms/step - loss: 9.5323e-04 - accuracy: 0.6032\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 9.2737e-04 - accuracy: 0.6126\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 16s 124ms/step - loss: 8.9798e-04 - accuracy: 0.6275\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 8.7258e-04 - accuracy: 0.6355\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 8.4280e-04 - accuracy: 0.6509\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 16s 119ms/step - loss: 8.2201e-04 - accuracy: 0.6590\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 21s 162ms/step - loss: 7.9460e-04 - accuracy: 0.6747\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 7.6806e-04 - accuracy: 0.6847\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 16s 122ms/step - loss: 7.4198e-04 - accuracy: 0.6986\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 18s 136ms/step - loss: 7.2576e-04 - accuracy: 0.7031\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 7.0499e-04 - accuracy: 0.7161\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 6.7986e-04 - accuracy: 0.7251\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 6.6328e-04 - accuracy: 0.7329\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 18s 139ms/step - loss: 6.4006e-04 - accuracy: 0.7458\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 16s 120ms/step - loss: 6.1383e-04 - accuracy: 0.7587\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 16s 124ms/step - loss: 5.9675e-04 - accuracy: 0.7657\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 16s 122ms/step - loss: 5.8076e-04 - accuracy: 0.7728\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 5.6090e-04 - accuracy: 0.7791\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 16s 121ms/step - loss: 5.4147e-04 - accuracy: 0.7885\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 18s 140ms/step - loss: 5.2863e-04 - accuracy: 0.7968\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 18s 139ms/step - loss: 5.1207e-04 - accuracy: 0.8056\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 4.8737e-04 - accuracy: 0.8169\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 16s 125ms/step - loss: 4.7351e-04 - accuracy: 0.8210\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 4.6021e-04 - accuracy: 0.8265\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 16s 120ms/step - loss: 4.3960e-04 - accuracy: 0.8358\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 16s 123ms/step - loss: 4.3130e-04 - accuracy: 0.8409\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 16s 124ms/step - loss: 4.1427e-04 - accuracy: 0.8482\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 16s 123ms/step - loss: 3.9595e-04 - accuracy: 0.8582\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 18s 136ms/step - loss: 3.8432e-04 - accuracy: 0.8622\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 3.7916e-04 - accuracy: 0.8647\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 3.6065e-04 - accuracy: 0.8722\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 17s 132ms/step - loss: 3.4929e-04 - accuracy: 0.8748\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 19s 140ms/step - loss: 3.4238e-04 - accuracy: 0.8803\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 3.2839e-04 - accuracy: 0.8841\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 3.1987e-04 - accuracy: 0.8919\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 19s 141ms/step - loss: 3.0018e-04 - accuracy: 0.8976\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 2.8578e-04 - accuracy: 0.9054\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 2.8482e-04 - accuracy: 0.9059\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 2.8558e-04 - accuracy: 0.9033\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 2.6012e-04 - accuracy: 0.9172\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 2.4293e-04 - accuracy: 0.9258\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 17s 125ms/step - loss: 2.4023e-04 - accuracy: 0.9244\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 16s 123ms/step - loss: 2.3889e-04 - accuracy: 0.9254\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 18s 133ms/step - loss: 2.2056e-04 - accuracy: 0.9346\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 2.2066e-04 - accuracy: 0.9325\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 16s 125ms/step - loss: 2.1391e-04 - accuracy: 0.9346\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 17s 125ms/step - loss: 2.1903e-04 - accuracy: 0.9311\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 16s 124ms/step - loss: 2.1653e-04 - accuracy: 0.9337\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 19s 141ms/step - loss: 2.0460e-04 - accuracy: 0.9378\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 1.8506e-04 - accuracy: 0.9470\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 1.7914e-04 - accuracy: 0.9480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4a46314610>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    result.append(seed_text)\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    saveprediction = [[0]]\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0] # model is predicting what comes after this\n",
    "        #print(len(encoded), in_text)\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        \n",
    "        # predict probabilities for each word\n",
    "        predictions = model.predict(encoded, verbose=0)[0]#[1:]\n",
    "        #print(len(predictions))\n",
    "        yhat = np.argmax(predictions)\n",
    "        #print(np.argmax(predictions[0]))\n",
    "        #yhat = random.randint(0, len(predictions))\n",
    "        #print(yhat)\n",
    "        \n",
    "#         yhat = model.predict(encoded, verbose=0)[0][1:]\n",
    "#         if np.array_equal(yhat, saveprediction):\n",
    "#             print(\"Nothing new\")\n",
    "#         saveprediction = yhat\n",
    "#         testList = list(yhat)\n",
    "#         indices = []\n",
    "#         for i in range(5):\n",
    "#             temp = np.argmax(testList)\n",
    "#             indices.append(temp)\n",
    "#             testList.pop(temp)\n",
    "#         yhat = np.argmax(yhat)\n",
    "#         yhat = random.choice(indices)\n",
    "\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            #print(\"check:\", word, index, yhat)\n",
    "            if index == yhat:\n",
    "                #print(\"out:\", word, index)\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a blonde arrives at work one morning blondes conceive consideration blondes conceive consideration consideration blondes conceive consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive consideration consideration blondes conceive\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"a blonde arrives at work one morning\"\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
